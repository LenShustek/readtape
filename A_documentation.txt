READTAPE: A program to recover data from old computer magnetic tapes 

OVERVIEW 

This software, still in development, analyzes the analog signal from a 
multi-track digital magnetic tape to reconstruct the original digital 
data. The motivation for starting with the analog data is to be able to 
recover data that has degraded to the point where conventional tape 
drives won't read it. 

The input to the "readtape" program is a sequence of periodic samples of 
the analog voltage from all channels of the read head as the tape is 
moving. The output is the reconstructed original data that was recorded
on the tape, and a log that indicates how successful it was.

We are currently using Qualstar 1052 tape drives to generate the data,
with either a 9-track or 7-track head installed. The drive manual is here: 
http://bitsavers.trailing-edge.com/pdf/qualstar/500150B_1052serviceMan.pdf.  
The analog samples are taken from the output of the second-stage 
amplifiers in the front end, which is pin 1 of Ux02A shown in the 
schematic on page 91. The drive is put in diagnostic read mode, which 
causes the tape to move past the head without needing to send any 
commands over the computer interface. 

The analog to digital conversion is from a Saleae 16-channel logic 
analyzer (https://www.saleae.com/), which does 12-bit digitization (4096 
possible values) of single-ended signals from -10V to +10V. Having 
bipolar inputs is important, because the output of the second operational 
amplifier is typically +-2V to +-6V. For degraded areas of the tapes it 
goes lower, but automatic gain control in the software allows it to 
decode signals that go as low as +-0.1V. 

The canonical order for the track data is: MSB...LSB, and then PARITY, 
connected to logical analyzer probes starting with number 0. If another 
order is used, the program can be told about that. 

I've used different sample rates depending on the density and drive speed, 
but I try to get about 20 samples per cycle. For 800 BPI NRZI at 50 IPS, 
the Saleae 781 Khz rate works well. The analyzer supports rates as high 
as 50 Mhz. 

The logic analyzer streams the data to a computer over a type 3.0 USB 
port. The data is stored in the computer's virtual memory, so the time 
and space limits are set by the memory+disk speed and size. Recording 
for many minutes to create files that take dozens of gigabytes hasn't 
been a problem. Files are saved in Saleae's proprietary format, and then
some of all of the file can exported to a text file.

The only form in which data can be exported from the Saleae analyzer 
software is a CSV (comma-separated-value) text file with a series of 
floating point numbers. The first number on each line is a timestamp in 
seconds, and the next N values on the lines are the voltages from the N 
tracks. The program can read that file. 

But the CSV files can be huge -- many tens of gigabytes for a few 
minutes of recording -- so for archival purposes we've defined a binary 
compressed "TBIN" format, and I wrote a utility program that can convert 
between CSV and TBIN. The compression factor is about 10:1, and the 
processing is about 2x faster when the program reads that file format.
There is some loss of precision in the compressed format, but it doesn't
affect the decoding.

USING THE PROGRAM 

This is a command-line (non-GUI) program written in standard C. I've 
been testing it on a Windows PC, but it should work in other 
environments too. 

use: readtape <options> <basefilename>
   the input file is <basefilename>.csv or .tbin
   the optional parameter set file is <basefilename>.parms
     (or NRZI.parms or PE.parms)
   the output files will be in the created directory <basefilename>\
   
options:
  -ntrks=n   set the number of tracks; default is 9
  -order=    set input data order for bits 0..ntrks-2 and P, where 0=MSB
             default is 01234567P for 9 trks, 012345P for 7 trks
  -pe        do PE decoding
  -nrzi      do NRZI decoding
  -gcr       do GCR decoding
  -ips=n     speed in inches/sec
  -bpi=n     density in bits/inch (default: autodetect)
  -even      expect even parity (for 7-track NRZI BCD tapes)
  -skip=n    skip the first n samples
  -tap       create one SIMH .tap file from all the data
  -deskew    do NRZI track deskew based on initial samples
  -addparity include the parity bit in the data, if ntrks<9
  -tbin      only look for a .tbin input file, not .csv
  -m         try multiple ways to decode a block
  -l         create a log file in the output directory
  -v         verbose mode (show all info)
  -t         terse mode (show only bad block info)
  -q         quiet mode (only say "ok" or "bad")
  -f         take a file list from <basefilename>.txt

By default, each set of records between tapemarks is stored as separate 
file. If it detects IBM standard labels, it uses those to name the files 
and doesn't write the labels themselves. In -tap mode, it instead creates
one tape image file in SIMH .tap format; see
http://simh.trailing-edge.com/docs/simh_magtape.pdf. 

Decoding is controlled by a set of parameters that adjust the 
algorithms. No one set of parameters will necessarily work for all tapes 
or even all blocks of one tape. You can specify multiple sets of 
parameters, and the program will try as many as necessary to get a 
perfect decoding of a block, or will try them all and pick the one that 
generates the minimum number of errors. 

The sets of parameters that are used for the decoding algorithms can 
be read from a file that is specific to the tape data being decoded, 
<basefilename>.parms, or from a generic file for that encoding type 
(NRZI.parms, PE.parms, etc.). If no parameter file is provided, the 
program uses built-in defaults. 

The algorithms are still a work in progress, so the parameter sets 
will continue to evolve. The current set of parameters are these: 

   int active;             // 1 means this is an active parameter set
   int clk_window;         // how many bit times to average for clock rate; 0 means maybe use exponential averaging
   float clk_alpha;        // weighting for current data in the clock rate exponential weighted average; 0 means use constant
   int agc_window;         // how many peaks to look back for the min peak to set AGC; 0 means maybe use exponential averaging
   float agc_alpha;        // weighting for current data in the AGC exponential weight average; 0 means no AGC
   float min_peak;         // the minimum height of a peak in volts
   float clk_factor;       // PE: how much of a half-bit period to wait for a clock transition
   float pulse_adj;        // PE: how much of the previous pulse's deviation to adjust this pulse by, 0 to 1
                           // NRZI: how much of the actual transition avg position to use to adjust the next expected
   float pkww_bitfrac;     // what fraction of the bit spacing the window width is
   char id[4];             // "PRM", to make sure the structure initialization isn't screwed up

The format of a parameter file is as follows:

//  comments
readtape <additional command line options>
parms active, clk_window,  clk_alpha, agc_window, agc_alpha, min_peak, clk_factor, pulse_adj, pkww_bitfrac, id
 {1,   0,     0.2,      5,    0.0,    0.0,   1.50,    0.4,   0.7, "PRM" }
 {1,   3,     0.0,      5,    0.0,    0.0,   1.40,    0.0,   0.7, "PRM" }
...

The leading 1 indicates an active parameter set, and the trailing "PRM" 
is for error checking. The numbers in between are the parameters, in the 
order given by the "parms" line of the file. 

If there is a now-obsolete parameter name in the file that the program 
no longer knows about, it is ignored with a warning. If a parameter name 
that the program expects is missing, the value from the first built-in 
parameter set is used. This scheme allows us to add and remove parameters 
types in the program without completely invalidating existing .parm 
files. 

Parameter sets are used in sequence for each block on the tape to find 
the best decoding. In verbose mode, the program reports on how many 
times each parameter set was successfully used to decode any block. 

There are other constant parameters for the algorithms that are defined 
in the decoder.h header file. As we discover any of them that need to 
have multiple values to decode all blocks we've encountered, they get 
moved into the parameter sets. 

UTILITY PROGRAMS

CSVTBIN: This standalone program converts between CSV format text files 
and TBIN format compressed binary files. 

use: csvtbin <options> <infilename> <outfilename>
options:
  -ntrks=n  the number of tracks; default is 9
  -order=   input data order for bits 0..ntrks-2 and P, where 0=MSB
            default is 01234567P for 9 trks, 012345P for 7 trks
  -skip=n   skip the first n samples
  -read     read tbin and create csv (otherwise, the opposite)
optional documentation that can be recorded in the TBIN file:
  -descr=txt            what is on the tape
  -pe                   PE enecoding
  -nrzi                 NRZI encding
  -gcr                  GCR enoding
  -ips=n                speed in inches/sec
  -bpi=n                density in bits/inch
  -datewritten=ddmmyyyy when the tape was originally written
  -dateread=ddmmyyyy    when the tape was read and digitized

Although not required, it is strongly suggested that the track order
be specified if it is not the standard MSB...LSB, PARITY.
By doing so the .tbin files will be in the canonical order, and
the readtape program won't need to be told about the non-standard order.

DUMPTAP: This standalone program displays the content of SIMH .tap 
format files with numbers in hex or octal, and/or characters in ASCII, 
EBCDIC, BCD, or Burroughs BIC code, in the style of an old-fashioned 
memory dump. 

use: dumptap <options> <filename>
  the input file is expected to be a SIMH .tap tape image
  the output is to stdout, but can be piped elsewhere
options:
  -b     show BCD characters
  -e     show EBCDIC characters
  -a     show ASCII characters
  -u     show Burroughs B5500 Internal Code characters
  -o     show octal numeric data
  -h     show hex numeric data
  -lnn   each line displays nn bytes

The default is 80 ASCII characters per line and no numeric data.
If the options are "-o -u -l20", the output looks like this:

  file:102784151.tap
     80: 6043212225436060004364422562606000232162   LABEL  0LUKES  0CAS
         6360606000000106110005010001061100050300  T   0016905101690530
         0000000000000000000000000000000000000000  00000000000000000000
         0000050600000005060000000000000000000000  00560005600000000000
  .tap tape mark
  .tap end of medium


THE ALGORITHMS

This system is still under development, so the algorithms will probably 
have changed by the time you read these descriptions. 

Peak Detection

As the tape moves past the head, a flux transition on any track creates 
a voltage peak, either positive or negative, in accordance with 
Faraday's Law, V = dB/dT. The amplitude of the voltage depends on many 
things: tape speed, bit density, head design, track alignment, the 
exact sequence of flux transitions, how well the tape was recorded, and
the condition of the tape.

We record the analog signal after a differential amplifier, a low-pass 
filter, and a differential to single-ended amplifier in the drive 
electronics. The signal is amazingly clean, with little noise or jitter 
that isn't attributable to the data on the tape. 

The first version of software peak detection used a hill-climbing 
algorithm to detect a local minimum or maximum. We kept track of whether 
we were going uphill or downhill, and recorded a peak when the direction 
changed by a non-trivial amount. The actual time of the peak was 
calculated by interpolating the times of samples that were close the the 
peak. 

The problem, especially when combined with the AGC algorithm described 
later, was the generation of false positives when the signal is small or 
when there are small variations. Tweaking the parameters to avoid that 
would suppress valid peaks. It would work when the parameters were 
carefully adjusted for a particular tape, but it was not robust. 

The second version of peak detection, which seems to work better, uses a 
moving window shape detector. We keep track of the last N samples for 
each track, of course using a circular buffer with head and tail 
pointers so the data doesn't have to be moved. As each new data point 
enters the window, we look to see if the maximum (minimum) sample is 
much higher (lower) than the oldest and newest values, and record a peak 
if so. We then become blind to peaks until that peak exits the window. 

The number of samples in the window is set dynamically based on the 
sampling rate, the expected bit rate, and a settable parameter. The 
amount by which a peak has to exceed the oldest and newest values is 
also parametrized and varies with the current AGC. The time of the peak 
is interpolated based on the previous and next sample values, so that if 
two or three values close to the peak are averaged. 

Compensation for "pulse shifting", which depends on the sequence and 
proximity of flux transitions, is done by computing the difference 
between the possible expected position of the pulse (which depends on 
the kind of encoding used) and the observed position. A parameter 
controls what fraction of that distance should be used to adjust the 
time of the peak before using it in the decoding algorithm. 

AGC: Automatic Gain Control

To compensate for differences in signal amplitude between tapes, between 
blocks, and between bits, we keep track of the average or expected peak 
heights for each track separately. The calculation produces an AGC value 
between 0.1 (for big signals) and 20 (for small signals). We change it by 
one of two algorithms: 

- exponential averaging: the new value is alpha * current + (1-alpha) * 
old average. This provide a kind of low-pass filter so that the gain
changes slowly. 

- recent minimum: the value is based on the minimum peak of the 
last N peaks we've seen. (Again, implemented with a circular buffer with 
head and tail pointers so stored data doesn't have to be moved.) This 
works better for some tapes where there are isolated very small peaks. 

The AGC value is used for the following two calculations: 

- choosing how high or low a voltage must be to be considered a peak 

- adjusting how close adjacent samples near the peak need to be for 
their times to be averaged 

High values of AGC let us detect peaks that are as small as 0.1V during 
dropouts on a particular track, yet still be able to ignore noise at 
other times when the signal is strong. 

Clock Recovery

...TBD..  window and exponential averaging here too..


PE Decoding Techniques

PE (phase encoding) has a negative flux transition for a 0-bit and a 
positive flux transition for a 1-bit. At the midpoint between bits, 
there is an optional flux transition if needed to prepare for the 
correct next bit flux transition. This is sometimes described as 
"Manchester" encoding. 

...TBD... setting the decision point...

NRZI Decoding Techniques

NRZI (non return to zero inverted) encoding has a flux transition of 
either direction for a 1-bit, and no transition for a 0-bit. Recovering 
the clock depends on there being frequent 1-bits in at least one track. 
My algorithm for NRZI decoding tolerates some track-to-track skew by 
working as follows: 

- Average the time that multiple tracks have "nearby" transitions, and 
use that as the assumed bit time. 

- The time between successive bit times establishes the data frequency, 
whose change over time is smoothed either by averaging over a fixed 
number of bits or by exponential weighting. 

- The time of any particular track's transition is compared to the 
expected bit time, and a settable fraction (50% seems to work well) of 
the deviation is used to adjust the transition time. (That tweak was 
added to compensate for data-dependent peak-shifting, but it helps
for modest amounts of track skew too.) 

- The decision about whether a track lacked a transition at a bit time 
isn't made until about halfway (0.6 seems to work well) between bit 
times. 

Automatic density detection

The recorded density of the tape is not always known. If the density is 
not specified by the -bpi option, the program will try to determine it 
by calculating the average minimum time between the first several thousand 
flux transitions. If it corresponds to one of the standard densities 
(200, 556, 800, 1600, or 6250), it will choose that for the decoding. 

Track skew compensation

If the head isn't perfectly vertical during either writing or reading, 
there will be consistent timing differences between data from the 
different tracks. If they are too big, it can cause errors in the 
decoding. 

If we need to insert track de-skewing data delays, they could be 
set by first reading a "master skew tape", if one is available. 
What we do instead is a statistical analysis of some of the 
transitions before trying to decode. See 
https://github.com/LenShustek/readtape/blob/master/flux_transition_dispersion.JPG
for an illustrative graph of data that shows track skew.

If the -deskew parameter is given, the program will attempt to calibrate 
the track skew by looking at the deviation between the actual times of 
the first several thousand flux transitions and the expected times. It 
then uses that to set up delays for the data from the heads whose 
transitions come early. 

One aspect of head alignment can't be fixed in software and is critical: 
we need a high enough signal-to-noise ratio so that the AGC algorithm 
can see the transitions, and minimal leakage between adjacent tracks. 


....more to come....

L. Shustek, 10 April 2018
L. Shustek, 17 May 2018


